{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1\n",
    "\n",
    "### Ayush Yadav (MDS202315)\n",
    "\n",
    "Build a prototype for sms spam classification <br>\n",
    "\n",
    "In `train.ipynb` write the functions to \n",
    "1) Fit a model on train data\n",
    "2) Score a model on given data\n",
    "3) Evaluate the model predictions \n",
    "4) Validate the model:\n",
    "\n",
    "   1) Fit on train\n",
    "   2) Score on train and validation\n",
    "   3) Evaluate on train and validation\n",
    "   4) Fine-tune hyper-params using train and validation (if necessary)\n",
    "5) Score three benchmark models on test data and select the best one \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score, confusion_matrix, roc_auc_score, roc_curve\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold, cross_val_score, train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "\n",
    "from scipy import sparse\n",
    "from scipy.sparse import load_npz\n",
    "\n",
    "import pickle\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN = pd.read_csv('./sms+spam+collection/TRAIN.csv').dropna()\n",
    "VAL = pd.read_csv('./sms+spam+collection/VALIDATION.csv').dropna()\n",
    "TEST = pd.read_csv('./sms+spam+collection/TEST.csv').dropna()\n",
    "\n",
    "bow_msgs = load_npz('./sms+spam+collection/bag_of_words.npz')\n",
    "\n",
    "with open('./sms+spam+collection/bag_of_words.pkl','rb') as f:\n",
    "    bag_of_words = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = TRAIN.drop(columns=['label'])\n",
    "train_y = TRAIN.label\n",
    "\n",
    "val_x = VAL.drop(columns=['label'])\n",
    "val_y = VAL.label\n",
    "\n",
    "test_x = TEST.drop(columns=['label'])\n",
    "test_y = TEST.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = bag_of_words.transform(train_x['preprocessed'])\n",
    "val_x = bag_of_words.transform(val_x['preprocessed'])\n",
    "test_x = bag_of_words.transform(test_x['preprocessed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (3785, 7947)\n",
      "Validation data shape: (947, 7947)\n",
      "Testing data shape: (834, 7947)\n"
     ]
    }
   ],
   "source": [
    "### Vectorizing using TF-IDF Vectorize\n",
    "\n",
    "tfidf_transformer = TfidfTransformer().fit(bow_msgs)\n",
    "\n",
    "X_train = tfidf_transformer.transform(train_x)\n",
    "print(\"Training data shape:\",X_train.shape)\n",
    "\n",
    "X_val = tfidf_transformer.transform(val_x)\n",
    "print(\"Validation data shape:\",X_val.shape)\n",
    "\n",
    "X_test = tfidf_transformer.transform(test_x)\n",
    "print(\"Testing data shape:\",X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fitting the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_bayes(X_train, y_train, X_val, y_val, X_test, y_test):\n",
    "\n",
    "    clf=MultinomialNB(fit_prior=True, class_prior=None)\n",
    "\n",
    "    clf_parameters = {\n",
    "        'clf__alpha':(0,1),\n",
    "        }\n",
    "\n",
    "    pipeline = Pipeline([('clf', clf)])\n",
    "\n",
    "    parameters={**clf_parameters}\n",
    "\n",
    "    grid = GridSearchCV(pipeline,parameters,n_jobs=-1,scoring='f1',cv=10)\n",
    "\n",
    "    grid.fit(X_train, y_train)\n",
    "\n",
    "    clf = grid.best_estimator_\n",
    "\n",
    "    val_preds = clf.predict(X_val)\n",
    "    test_preds = clf.predict(X_test)\n",
    "\n",
    "    print('='*100)\n",
    "    print('Performance on validation set\\n'.upper())\n",
    "    print(\"Accuracy: \",accuracy_score(y_val, val_preds))\n",
    "    print(\"F1 Score: \",f1_score(y_val, val_preds))\n",
    "    print(classification_report(y_val, val_preds))\n",
    "    print('='*100)\n",
    "    print('Performance on test set\\n'.upper())\n",
    "    print(\"Accuracy: \",accuracy_score(y_test, test_preds))\n",
    "    print(\"F1 Score: \",f1_score(y_test, test_preds))\n",
    "    print(classification_report(y_test, test_preds))\n",
    "    print('='*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "PERFORMANCE ON VALIDATION SET\n",
      "\n",
      "Accuracy:  0.9640971488912354\n",
      "F1 Score:  0.8454545454545455\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.98       820\n",
      "           1       1.00      0.73      0.85       127\n",
      "\n",
      "    accuracy                           0.96       947\n",
      "   macro avg       0.98      0.87      0.91       947\n",
      "weighted avg       0.97      0.96      0.96       947\n",
      "\n",
      "====================================================================================================\n",
      "PERFORMANCE ON TEST SET\n",
      "\n",
      "Accuracy:  0.9532374100719424\n",
      "F1 Score:  0.7891891891891892\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.97       722\n",
      "           1       1.00      0.65      0.79       112\n",
      "\n",
      "    accuracy                           0.95       834\n",
      "   macro avg       0.97      0.83      0.88       834\n",
      "weighted avg       0.96      0.95      0.95       834\n",
      "\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "naive_bayes(X_train, train_y, X_val, val_y, X_test, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(X_train, y_train, X_val, y_val, X_test, y_test):\n",
    "\n",
    "    clf = LogisticRegression(class_weight='balanced', n_jobs=-1)\n",
    "    clf_parameters = {\n",
    "                    'clf__solver':('newton-cg','lbfgs','liblinear','saga'),\n",
    "                }\n",
    "\n",
    "    pipeline = Pipeline([('clf', clf)])\n",
    "\n",
    "    parameters={**clf_parameters}\n",
    "\n",
    "    grid = GridSearchCV(pipeline,parameters,n_jobs=-1,scoring='f1',cv=10)\n",
    "\n",
    "    grid.fit(X_train, y_train)\n",
    "\n",
    "    clf = grid.best_estimator_\n",
    "\n",
    "    val_preds = clf.predict(X_val)\n",
    "    test_preds = clf.predict(X_test)\n",
    "\n",
    "    print('='*100)\n",
    "    print('Performance on validation set\\n'.upper())\n",
    "    print(\"Accuracy: \",accuracy_score(y_val, val_preds))\n",
    "    print(\"F1 Score: \",f1_score(y_val, val_preds))\n",
    "    print(classification_report(y_val, val_preds))\n",
    "    print('='*100)\n",
    "    print('Performance on test set\\n'.upper())\n",
    "    print(\"Accuracy: \",accuracy_score(y_test, test_preds))\n",
    "    print(\"F1 Score: \",f1_score(y_test, test_preds))\n",
    "    print(classification_report(y_test, test_preds))\n",
    "    print('='*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "PERFORMANCE ON VALIDATION SET\n",
      "\n",
      "Accuracy:  0.9704329461457233\n",
      "F1 Score:  0.889763779527559\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98       820\n",
      "           1       0.89      0.89      0.89       127\n",
      "\n",
      "    accuracy                           0.97       947\n",
      "   macro avg       0.94      0.94      0.94       947\n",
      "weighted avg       0.97      0.97      0.97       947\n",
      "\n",
      "====================================================================================================\n",
      "PERFORMANCE ON TEST SET\n",
      "\n",
      "Accuracy:  0.9688249400479616\n",
      "F1 Score:  0.8818181818181818\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98       722\n",
      "           1       0.90      0.87      0.88       112\n",
      "\n",
      "    accuracy                           0.97       834\n",
      "   macro avg       0.94      0.93      0.93       834\n",
      "weighted avg       0.97      0.97      0.97       834\n",
      "\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "logistic_regression(X_train, train_y, X_val, val_y, X_test, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_forest(X_train, y_train, X_val, y_val, X_test, y_test):\n",
    "\n",
    "    clf = RandomForestClassifier(class_weight='balanced', max_depth=10)\n",
    "    clf_parameters = {\n",
    "                'clf__criterion':('gini', 'entropy'), \n",
    "                'clf__max_features':('sqrt', 'log2'),   \n",
    "                'clf__n_estimators':(10, 30,50,100,200),\n",
    "                'clf__max_depth':(10,20),\n",
    "                } \n",
    "\n",
    "    pipeline = Pipeline([('clf', clf)])\n",
    "\n",
    "    parameters={**clf_parameters}\n",
    "\n",
    "    grid = GridSearchCV(pipeline,parameters,n_jobs=-1,scoring='f1',cv=10)\n",
    "\n",
    "    grid.fit(X_train, y_train)\n",
    "\n",
    "    clf = grid.best_estimator_\n",
    "\n",
    "    val_preds = clf.predict(X_val)\n",
    "    test_preds = clf.predict(X_test)\n",
    "\n",
    "    print('='*100)\n",
    "    print('Performance on validation set\\n'.upper())\n",
    "    print(\"Accuracy: \",accuracy_score(y_val, val_preds))\n",
    "    print(\"F1 Score: \",f1_score(y_val, val_preds))\n",
    "    print(classification_report(y_val, val_preds))\n",
    "    print('='*100)\n",
    "    print('Performance on test set\\n'.upper())\n",
    "    print(\"Accuracy: \",accuracy_score(y_test, test_preds))\n",
    "    print(\"F1 Score: \",f1_score(y_test, test_preds))\n",
    "    print(classification_report(y_test, test_preds))\n",
    "    print('='*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "PERFORMANCE ON VALIDATION SET\n",
      "\n",
      "Accuracy:  0.9757127771911299\n",
      "F1 Score:  0.9037656903765691\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99       820\n",
      "           1       0.96      0.85      0.90       127\n",
      "\n",
      "    accuracy                           0.98       947\n",
      "   macro avg       0.97      0.92      0.94       947\n",
      "weighted avg       0.98      0.98      0.98       947\n",
      "\n",
      "====================================================================================================\n",
      "PERFORMANCE ON TEST SET\n",
      "\n",
      "Accuracy:  0.973621103117506\n",
      "F1 Score:  0.8921568627450981\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.98       722\n",
      "           1       0.99      0.81      0.89       112\n",
      "\n",
      "    accuracy                           0.97       834\n",
      "   macro avg       0.98      0.91      0.94       834\n",
      "weighted avg       0.97      0.97      0.97       834\n",
      "\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "random_forest(X_train, train_y, X_val, val_y, X_test, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_boost(X_train, y_train, X_val, y_val, X_test, y_test):\n",
    "\n",
    "    clf = GradientBoostingClassifier()\n",
    "    clf_parameters = {\n",
    "                'clf__loss':('log_loss','exponential'),        \n",
    "                'clf__criterion':('friedman_mse', 'squared_error'), \n",
    "                'clf__max_features':('sqrt', 'log2'),   \n",
    "                'clf__n_estimators':(50,100,200),\n",
    "                'clf__max_depth':(5,10),\n",
    "                } \n",
    "\n",
    "    pipeline = Pipeline([('clf', clf)])\n",
    "\n",
    "    parameters={**clf_parameters}\n",
    "\n",
    "    grid = GridSearchCV(pipeline,parameters,n_jobs=-1,scoring='f1',cv=10)\n",
    "\n",
    "    grid.fit(X_train, y_train)\n",
    "\n",
    "    clf = grid.best_estimator_\n",
    "\n",
    "    val_preds = clf.predict(X_val)\n",
    "    test_preds = clf.predict(X_test)\n",
    "\n",
    "    print('='*100)\n",
    "    print('Performance on validation set\\n'.upper())\n",
    "    print(\"Accuracy: \",accuracy_score(y_val, val_preds))\n",
    "    print(\"F1 Score: \",f1_score(y_val, val_preds))\n",
    "    print(classification_report(y_val, val_preds))\n",
    "    print('='*100)\n",
    "    print('Performance on test set\\n'.upper())\n",
    "    print(\"Accuracy: \",accuracy_score(y_test, test_preds))\n",
    "    print(\"F1 Score: \",f1_score(y_test, test_preds))\n",
    "    print(classification_report(y_test, test_preds))\n",
    "    print('='*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "PERFORMANCE ON VALIDATION SET\n",
      "\n",
      "Accuracy:  0.9725448785638859\n",
      "F1 Score:  0.8859649122807017\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.98       820\n",
      "           1       1.00      0.80      0.89       127\n",
      "\n",
      "    accuracy                           0.97       947\n",
      "   macro avg       0.98      0.90      0.94       947\n",
      "weighted avg       0.97      0.97      0.97       947\n",
      "\n",
      "====================================================================================================\n",
      "PERFORMANCE ON TEST SET\n",
      "\n",
      "Accuracy:  0.9664268585131894\n",
      "F1 Score:  0.8571428571428571\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.98       722\n",
      "           1       1.00      0.75      0.86       112\n",
      "\n",
      "    accuracy                           0.97       834\n",
      "   macro avg       0.98      0.88      0.92       834\n",
      "weighted avg       0.97      0.97      0.96       834\n",
      "\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "grad_boost(X_train, train_y, X_val, val_y, X_test, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SVM(X_train, y_train, X_val, y_val, X_test, y_test):\n",
    "\n",
    "    clf = SVC(class_weight='balanced')  \n",
    "    clf_parameters = {\n",
    "        'clf__C':(0.1,0.5,1,2,10,50,100),\n",
    "        'clf__kernel': ('linear', 'rbf','poly')\n",
    "        }\n",
    "\n",
    "    pipeline = Pipeline([('clf', clf)])\n",
    "\n",
    "    parameters={**clf_parameters}\n",
    "\n",
    "    grid = GridSearchCV(pipeline,parameters,n_jobs=-1,scoring='f1',cv=10)\n",
    "\n",
    "    grid.fit(X_train, y_train)\n",
    "\n",
    "    clf = grid.best_estimator_\n",
    "\n",
    "    val_preds = clf.predict(X_val)\n",
    "    test_preds = clf.predict(X_test)\n",
    "\n",
    "    print('='*100)\n",
    "    print('Performance on validation set\\n'.upper())\n",
    "    print(\"Accuracy: \",accuracy_score(y_val, val_preds))\n",
    "    print(\"F1 Score: \",f1_score(y_val, val_preds))\n",
    "    print(classification_report(y_val, val_preds))\n",
    "    print('='*100)\n",
    "    print('Performance on test set\\n'.upper())\n",
    "    print(\"Accuracy: \",accuracy_score(y_test, test_preds))\n",
    "    print(\"F1 Score: \",f1_score(y_test, test_preds))\n",
    "    print(classification_report(y_test, test_preds))\n",
    "    print('='*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "PERFORMANCE ON VALIDATION SET\n",
      "\n",
      "Accuracy:  0.9725448785638859\n",
      "F1 Score:  0.8925619834710744\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       820\n",
      "           1       0.94      0.85      0.89       127\n",
      "\n",
      "    accuracy                           0.97       947\n",
      "   macro avg       0.96      0.92      0.94       947\n",
      "weighted avg       0.97      0.97      0.97       947\n",
      "\n",
      "====================================================================================================\n",
      "PERFORMANCE ON TEST SET\n",
      "\n",
      "Accuracy:  0.9772182254196643\n",
      "F1 Score:  0.9107981220657277\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       722\n",
      "           1       0.96      0.87      0.91       112\n",
      "\n",
      "    accuracy                           0.98       834\n",
      "   macro avg       0.97      0.93      0.95       834\n",
      "weighted avg       0.98      0.98      0.98       834\n",
      "\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "SVM(X_train, train_y, X_val, val_y, X_test, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|          Model         | Validation Accuracy | Validation F1 Score | Test Accuracy | Test F1 Score |\n",
    "|:----------------------:|---------------------|---------------------|---------------|---------------|\n",
    "| Naive Bayes            | 0.96                | 0.85                | 0.95          | 0.79          |\n",
    "| Logistic Regression    | 0.97                | 0.89                | 0.97          | 0.88          |\n",
    "| Random Forest          | 0.97                | 0.90                | 0.97          | 0.89          |\n",
    "| Gradient Boosting      | 0.97                | 0.89                | 0.97          | 0.86          |\n",
    "| Support Vector Machine | 0.97                | 0.89                | 0.98          | 0.91          |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Best Model (acc to F1 Score)**: Support Vector Machine Classifier"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
